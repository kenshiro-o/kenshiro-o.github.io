<!DOCTYPE html>
<html lang="en-GB" />
<head>
    <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />

    <title>Using Computer Vision to Find Lane Lines on Road &middot; </title>

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="/favicon.ico" />
    <link rel="canonical" href="/posts/using-computer-vision-to-find-lane-lines-on-road/" />

     <meta name="description" content="Identifying lanes on the road is a common task performed by all human drivers to ensure their vehicles are within lane constraints when driving, so as to make s" /> 

     
    
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image" content="/img/post_covers/basic_lane_line_detection.png"/>
    
 
    <meta name="twitter:title" content="Using Computer Vision to Find Lane Lines on Road"/>
    <meta name="twitter:description" content="Identifying lanes on the road is a common task performed by all human drivers to ensure their vehicles are within lane constraints when driving, so as to make s"/>
    <meta name="twitter:url" content="/posts/using-computer-vision-to-find-lane-lines-on-road/" />
    <meta name="twitter:site" content="@Ed_Forson"/>

    <meta property="og:site_name" content="" />
    <meta property="og:title" content="Using Computer Vision to Find Lane Lines on Road &middot; Eddie&#39;s Blog" />
    <meta property="og:url" content="/posts/using-computer-vision-to-find-lane-lines-on-road/" />
    

    <meta property="og:type" content="article" />
    <meta property="og:description" content="Identifying lanes on the road is a common task performed by all human drivers to ensure their vehicles are within lane constraints when driving, so as to make s" />

    <meta property="article:published_time" content="2017-07-25T16:11:24&#43;01:00" />
    <meta property="article:tag" content="AI" /><meta property="article:tag" content="computer-vision" /><meta property="article:tag" content="self-driving-cars" />

    <meta property="og:image" content="/img/post_covers/basic_lane_line_detection.png"/>


    <meta name="generator" content="Hugo 0.47.1" />

    <!-- Stylesheets -->
    <link rel="stylesheet" type="text/css" href="/built/screen.css" /> 
    <link rel="stylesheet" type="text/css" href="/css/casper-two.css" /> 
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" />
     <link rel="stylesheet" href="/css/overrides.css" /> 

     

</head>


<body class="post-template">
  <div class="site-wrapper"> 

<header class="site-header outer">
  <div class="inner">
    <nav class="site-nav">
      <div class="site-nav-left">

        <ul class="nav" role="menu">
        
        
        
            <li class="" role="menuitem">
              <a href="/">Home</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/tags/software/">Software</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/categories/ai/">AI</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/categories/self-driving-cars/">Self-Driving Cars</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/categories/life/">Life</a>
            </li>
        
      </ul></div>

      <div class="site-nav-right">
        <div class="social-links">
                    

                    <a class="social-link social-link-tw" href="https://twitter.com/Ed_Forson" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg></a>

                    <a class="social-link" href="https://github.com/kenshiro-o" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a>

                    <a class="social-link" href="https://www.linkedin.com/in/eddie-forson-16269b22" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 50 512 512"><path d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683 C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615 c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915 s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z" /></svg></a>

                    <a class="social-link" href="https://medium.com/@Ed_Forson" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 195 195"><path d="M46.5340803,65.2157554 C46.6968378,63.6076572 46.0836,62.018231 44.8828198,60.93592 L32.6512605,46.2010582 L32.6512605,44 L70.6302521,44 L99.9859944,108.380952 L125.794585,44 L162,44 L162,46.2010582 L151.542017,56.2281011 C150.640424,56.9153477 150.193188,58.0448862 150.380019,59.1628454 L150.380019,132.837155 C150.193188,133.955114 150.640424,135.084652 151.542017,135.771899 L161.755369,145.798942 L161.755369,148 L110.38282,148 L110.38282,145.798942 L120.963119,135.527337 C122.002801,134.487948 122.002801,134.182246 122.002801,132.592593 L122.002801,73.0417402 L92.585901,147.755438 L88.6106443,147.755438 L54.3622782,73.0417402 L54.3622782,123.115814 C54.0767278,125.221069 54.7759199,127.3406 56.2581699,128.863022 L70.0186741,145.55438 L70.0186741,147.755438 L31,147.755438 L31,145.55438 L44.7605042,128.863022 C46.2319621,127.338076 46.8903838,125.204485 46.5340803,123.115814 L46.5340803,65.2157554 Z"/></svg></a>
        </div>  
            
      </div>

    </nav>  

  </div>
</header>

<main id="site-main" class="site-main outer" role="main">
  <div class="inner">
    
      <article class="post-full post"> 
    <header class="post-full-header">
        <section class="post-full-meta">
            <time class="post-full-meta-date" datetime="2017-07-25">25 July 2017</time>
                <span class="date-divider">/</span> <a href="/tags/ai/">#AI</a>&nbsp;<a href="/tags/computer-vision/">#computer-vision</a>&nbsp;
        </section>
        <h1 class="post-full-title">Using Computer Vision to Find Lane Lines on Road</h1>
    </header>
    
    <figure class="post-full-image" style="background-image: url(/img/post_covers/basic_lane_line_detection.png)">
    </figure>

    <section class="post-full-content">
        <div class="kg-card-markdown">
        

<p>Identifying lanes on the road is a common task performed by all human drivers to ensure their vehicles are within lane constraints when driving, so as to make sure traffic is smooth and minimize chances of collisions with other cars due to lane misalignment.</p>

<p>Similarly, it is a critical task for an autonomous vehicle to perform. It turns out that recognizing lane markings on roads is possible using well known computer vision techniques. Some of those techniques will be covered below.</p>

<p>The goal of this project, the first of Term 1 of the Udacity Self Driving Car Engineer Nanodegree, is to create a pipeline that finds lane lines on the road using Python and OpenCV.</p>

<h2 id="setup">Setup</h2>

<p>Udacity provided sample images of 960 x 540 pixels to train our pipeline against. Below are two of the provided images.</p>


<figure class="figure">
    
        <img src="/img/computer_vision_lane_line_detection/white_vs_yellow_lane_lines.png" alt="Solid white lane lines on left and yellow lane lines on right" />
    
    
    <figcaption>
        <p>
        White vs Yellow Lane Lines
        
            
        
        </p> 
    </figcaption>
    
</figure>


<h2 id="the-pipeline">The Pipeline</h2>

<p>In this part, we will cover in detail the different steps needed to create our pipeline, which will enable us to identify and classify lane lines. The pipeline itself will look as follows:</p>

<p>Convert original image to HSL
Isolate yellow and white from HSL image
Combine isolated HSL with original image
Convert image to grayscale for easier manipulation
Apply Gaussian Blur to smoothen edges
Apply Canny Edge Detection on smoothed gray image
Trace Region Of Interest and discard all other lines identified by our previous step that are outside this region
Perform a Hough Transform to find lanes within our region of interest and trace them in red
Separate left and right lanes
Interpolate line gradients to create two smooth lines
The input to each step is the output of the previous step (e.g. we apply Hough Transform to region segmented image).</p>

<h3 id="convert-to-different-color-spaces">Convert To Different Color Spaces</h3>

<p>While our image is currently in RBG format, we should explore whether visualizing it in different color spaces such as HSL or HSV to see whether they can help us in better isolating the lanes. Note that HSV is often referred to as HSB (Hue Saturation and Brightness). I was trying to get my head around the major differences between these two color codes and came across this resource the other day which summed it quite well:</p>

<blockquote>
<p>HSL is slightly different. Hue takes exactly the same numerical value as in HSB/HSV. However, S, which also stands for Saturation, is defined differently and requires conversion. L stands for Lightness, is not the same as Brightness/Value. Brightness is perceived as the “amount of light” which can be any color while Lightness is best understood as the amount of white. Saturation is different because in both models is scaled to fit the definition of brightness/lightness.</p>
</blockquote>

<p>The diagrams below enable one to visualize the differences between the two:</p>


<figure class="figure">
    
        <img src="/img/computer_vision_lane_line_detection/hsl_hsv.png" alt="HSL and HSV color spaces" />
    
    
    <figcaption>
        <p>
        HSL and HSV color spaces
        
            
        
        </p> 
    </figcaption>
    
</figure>


<p>The image below shows the original image next to its HSV and HSL equivalents:</p>


<figure class="figure">
    
        <img src="/img/computer_vision_lane_line_detection/original_hsv_hsl.png" alt="Original vs HSV vs HSL images" />
    
    
    <figcaption>
        <p>
        Original vs HSV vs HSL
        
            
        
        </p> 
    </figcaption>
    
</figure>


<p>As can be seen while comparing images, HSL is better at contrasting lane lines than HSV. HSV is “blurring” our white lines too much, so it would not be suitable for us to opt for it in this case. At the very least it will be easier for us to isolate yellow and white lanes using HSL. So let’s use it.</p>

<h3 id="isolating-yellow-and-white-from-hsl-image">Isolating Yellow And White From HSL Image</h3>

<p>We first isolate yellow and white from the original image. After doing so, we can observe how the yellow and the white of the lanes are very well isolated.</p>


<figure class="figure">
    
        <img src="/img/computer_vision_lane_line_detection/original_hsl_yellow_white_line_filters.png" alt="Original image vs HSL yellow and white line filters" />
    
    
    <figcaption>
        <p>
        Original image vs HSL Yellow and White Line Filters
        
            
        
        </p> 
    </figcaption>
    
</figure>


<p>Let’s now combine those two masks using an OR operation and then combine with the original image using an AND operation to only retain the intersecting elements.</p>

<p>The results are very satisfying so far. See how the yellow road signs are clearly identified thanks to our HSL yellow mask! Next we move to grayscaling the image.</p>

<h2 id="convert-to-grayscale">Convert To Grayscale</h2>

<p>We are interested in detecting white or yellow lines on images, which show a particularly high contrast when the image is in grayscale. Remember that the road is black, so anything that is much brighter on the road will come out with a high contrast in a grayscale image.</p>

<p>The conversion from HSL to grayscale helps in reducing noise even further. This is also a necessary pre-processing step before we can run more powerful algorithms to isolate lines.</p>


<figure class="figure">
    
        <img src="/img/computer_vision_lane_line_detection/combined_hsl_grayscale.png" alt="Combined HSL Images With Grayscale" />
    
    
    <figcaption>
        <p>
        Combined HSL Images With Grayscale
        
            
        
        </p> 
    </figcaption>
    
</figure>


<h2 id="gaussian-blur">Gaussian Blur</h2>

<p>Gaussian blur (also referred to as Gaussian smoothing) is a pre-processing technique used to smoothen the edges of an image to reduce noise. We counter-intuitively take this step to reduce the number of lines we detect, as we only want to focus on the most significant lines (the lane ones), not those on every object. We must be careful as to not blur the images too much otherwise it will become hard to make up a line.</p>

<p>The OpenCV implementation of Gaussian Blur takes a integer kernel parameter which indicates the intensity of the smoothing. For our task we choose a value of 11.</p>

<p>The images below show what a typical Gaussian blur does to an image, the original image is on the left while the blurred one is to its right.</p>


<figure class="figure">
    
        <img src="/img/computer_vision_lane_line_detection/grayscale_vs_gaussian_blur.png" alt="Grayscale (left image) vs Gaussian Blur (right image)" />
    
    
    <figcaption>
        <p>
        Grayscale (left) vs Gaussian Blur (right)
        
            
        
        </p> 
    </figcaption>
    
</figure>


<h2 id="canny-edge-detection">Canny Edge Detection</h2>

<p>Now that we have sufficiently pre-processed the image, we can apply a Canny Edge Detector, whose role it is to identify edges in an image and discard all other data. The resulting image ends up being wiry, which enables us to focus on lane detection even more, since we are concerned with lines.</p>

<p>The OpenCV implementation requires passing in two parameters in addition to our blurred image, a low and high threshold which determines whether to include a given edge or not. A threshold captures the intensity of change of a given point (you can think of it as a gradient). Any point beyond the high threshold will be included in our resulting image, while points between the threshold values will only be included if they are next to edges beyond our high threshold. Edges that are below our low threshold are discarded. Recommended low:high threshold ratios are 1:3 or 1:2. We use values 50 and 150 respectively for low and high thresholds.</p>

<p>We show the smoothened grayscale and canny images together below:</p>


<figure class="figure">
    
        <img src="/img/computer_vision_lane_line_detection/grayscale_vs_canny_edge.png" alt="Grayscale (left image) vs Canny Edge (right image)" />
    
    
    <figcaption>
        <p>
        Grayscale (left) vs Canny Edge (right)
        
            
        
        </p> 
    </figcaption>
    
</figure>


<h2 id="region-of-interest">Region Of Interest</h2>

<p>Our next step is to determine a region of interest and discard any lines outside of this polygon. One crucial assumption in this task is that the camera remains in the same place across all these image, and lanes are flat, therefore we can identify the critical region we are interested in.</p>

<p>Looking at the above images, we “guess” what that region may be by following the contours of the lanes the car is in and define a polygon which will act as our region of interest below.</p>

<p>We put the canny and segmented images side by side and observe how only the most relevant details have been conserved:</p>


<figure class="figure">
    
        <img src="/img/computer_vision_lane_line_detection/canny_vs_segmented_canny.png" alt="Canny Edge (left image) vs Segmented Canny Edge (right image)" />
    
    
    <figcaption>
        <p>
        Canny Edge (left) vs Segmented Canny Edge (right)
        
            
        
        </p> 
    </figcaption>
    
</figure>


<h2 id="hough-transform">Hough Transform</h2>

<p>The next step is to apply the Hough Transform technique to extract lines and color them. The goal of Hough Transform is to find lines by identifying all points that lie on them. This is done by converting our current system denoted by axis (x,y) to a parametric one where axes are (m, b). In this plane:</p>

<p>lines are represented as points
points are presented as lines (since they can be on many lines in traditional coordinate system)
intersecting lines means the same point is on multiple lines
Therefore, in such plane, we can more easily identify lines that go via the same point. We however need to move from the current system to a Hough Space which uses a polar coordinates system as our original expression is not differentiable when m=0 (i.e. vertical lines). In polar coordinates, a given line will now be expressed as (ρ, θ), where line L is reachable by going a distance ρ at angle θ from the origin, thus meeting the perpendicular L; that is ρ = x cos θ + y sin θ.
All straight lines going through a given point will correspond to a sinusoidal curve in the (ρ, θ) plane. Therefore, a set of points on the same straight line in Cartesian space will yield sinusoids that cross at the point (ρ, θ). This naturally means that the problem of detecting points on a line in cartesian space is reduced to finding intersecting sinusoids in Hough space.</p>


<figure class="figure">
    
        <img src="/img/computer_vision_lane_line_detection/hough_transform_diagram.png" alt="Hough Space" />
    
    
    <figcaption>
        <p>
        Hough Space
        
            
        
        </p> 
    </figcaption>
    
</figure>


<p>More information about the implementation of Hough Transform in OpenCV can be found here.</p>

<p>The Hough transform returns lines, and the below images show what they look like:</p>


<figure class="figure">
    
        <img src="/img/computer_vision_lane_line_detection/hough_lines_red.png" alt="Hough Lane Lines Highlighted In Red" />
    
    
    <figcaption>
        <p>
        Hough Lane Lines Highlighted In Red
        
            
        
        </p> 
    </figcaption>
    
</figure>


<h2 id="separating-left-and-right-lanes">Separating Left And Right lanes</h2>

<p>To be able to trace a full line and connect lane markings on the image, we must be able to distinguish left from right lanes. Fortunately, there is a trivial way to do so. If you carefully look the image (may be easier with the canny segmented images), you can derive the gradient (i.e slope) of any left or right lane line:</p>

<p>left lane: as x value (i.e. width) increases, y value (i.e. height) decreases: slope must thus be negative
right lane: as x value (i.e. width) increases, y value (i.e. height) increases: slope must thus be positive
We can therefore define a function that separates lines into a left and right one. We must be careful when the denominator of the gradient (the dx in dy/dx) is 0, and ignore any line with such line.</p>

<p>In the below images, we color identified lines belonging to the left lane in red, while those belonging to the right lane are in blue:</p>


<figure class="figure">
    
        <img src="/img/computer_vision_lane_line_detection/lane_lines_red_blue.png" alt="Left Lane Red, Right Lane Blue" />
    
    
    <figcaption>
        <p>
        Left Lane Red, Right Lane Blue
        
            
        
        </p> 
    </figcaption>
    
</figure>


<h2 id="gradient-interpolation-and-line-extrapolation">Gradient Interpolation and Line Extrapolation</h2>

<p>To trace a full line from the bottom of the screen to the highest point of our region of interest, we must be able to interpolate the different points returned by our Hough transform function, and find a line that minimizes the distance across those points. Basically this is a linear regression problem. We will attempt to find the line on a given lane by minimizing the least squares error. We conveniently use the scipy.stats.linregress(x, y) function to find the slope and intercept of our lane line.</p>

<p>We succeed in doing so, as attested by the following images below:</p>


<figure class="figure">
    
        <img src="/img/computer_vision_lane_line_detection/full_lane_lines_identified.png" alt="Full Lane Line Identified" />
    
    
    <figcaption>
        <p>
        Full Lane Lines Identifiesd
        
            
        
        </p> 
    </figcaption>
    
</figure>


<h2 id="videos">Videos</h2>

<h3 id="setup-1">Setup</h3>

<p>Three videos were also provided to run our pipeline against them:</p>

<p>a 10 seconds video with only white lane lines
a 27 seconds video with a continuous yellow lane line on the left and dotted white lane line on right
a challenge video where the road is slightly curved and the resolution of frames is higher</p>

<h3 id="first-implementation">First implementation</h3>

<p>The initial implementation worked passably on the first two videos but utterly failed on the challenge exercise. To make the line detection smoother and take advantage of the sequencing and locality of each frame (and therefore lines), I decided to interpolate lane gradients and intercepts across frames, and discard any line that deviated too much from the computed mean from previous frames.</p>

<h3 id="lane-detector-with-memory-of-previous-frames">Lane Detector With Memory Of Previous Frames</h3>

<p>Remember that a video is a sequence of frames. We can therefore use the information from previous frames to smoothen the lines that we trace on the road and take corrective steps if at frame t our computed lines differ disproportionately from the mean of line slopes and intercepts we computed from frames [0, t-1].</p>

<p>We therefore need to impart the concept of memory into our pipeline. We will use a standard Python deque to store the last N (I set it at 15 for now) computed line coefficients.</p>

<p>This worked fairly well on the first two video and even managed to honorably detect lane lines on the challenge video, but because of the curvature of the lanes a straight line formed by a simple polynomial of degree 1 (i.e. y = Ax¹ + b) would not be enough. In the future I’ll work on better fitting lane lines on curvy roads.</p>

<p>The video below shows the pipeline in action on the challenge video:</p>

<p><div class="fluid-width-video-container"><div class="fluid-width-video-wrapper" style="padding-top: 56.25%;"><iframe src="//www.youtube.com/embed/qiNSKqtJ1Eo?autoplay=0" allowfullscreen="" title="YouTube Video" name="fitvid0"></iframe></div></div></p>

<h2 id="shortcomings">Shortcomings</h2>

<p>I have observed some problems with the current pipeline:</p>

<ul>
<li>in the challenge video at around second 5 the lane is covered by some shadow and my code originally failed to detect it. I managed to fix this issue by applying the HSL color filtering as another pre-processing step.</li>
<li>straight lines do not work when there are curves on the road :)</li>
<li>Hough Transform is tricky to get right with its parameters. I am not sure I got the best settings</li>
</ul>

<h2 id="future-improvements">Future Improvements</h2>

<p>One further step to explore would be to calculate the weighted average of line coefficients in our MemoryLaneDetector, giving a higher weight to more recent coefficients as they belong to more recent frames; I believe frame locality would play a critical role in getting near-perfect lines on video.</p>

<p>We should also consider expressing lines as second degree polynomials or more for examples such as the challenge video.</p>

<p>In the future, I also plan to use deep learning to identify lanes and compare those results against what I obtained with a pure computer vision approach.</p>

<p>All code is available on <a href="https://github.com/kenshiro-o/CarND-LaneLines-P1">Github</a>.</p>

<p>This was an exciting and challenging first project that got me to understand a lot more about color spaces, image processing and revise some linear algebra too. I am looking forward to even more challenging projects that stretch my knowledge in all these fields and more, and help me in understanding how a self-driving car is built!</p>
    
        </div>
    </section>

    <footer class="post-full-footer">
      <section class="author-card">
        <img class="author-profile-image" src="/img/eddie_india_small.jpeg" alt="Author" />
        <section class="author-card-content">
            <h4 class="author-card-name"><a href="/">Eddie Forson</a></h4>
                <p>UK-based, French software engineer with experience in product management. Curious. Very much into AI and self-driving cars these days.</p>
        </section>
      </section>
    </footer>
</article>
    
    
    

  </div>
</main>


<aside class="read-next outer">
  <div class="inner">
    <div class="read-next-feed">      
      

      
      
      <article class="post-card post"> 
    
    <a class="post-card-image-link" href="/posts/3-ways-to-spend-money-to-improve-quality-of-life/">
      <div class="post-card-image" style="background-image: url(/img/post_covers/money_euros_pyramid.jpeg)"></div>
    </a>
    

    <div class="post-card-content">
      <a class="post-card-content-link" href="/posts/3-ways-to-spend-money-to-improve-quality-of-life/">
          <header class="post-card-header">
              <span class="post-card-tags">
              #life 
              #self-improvement 
              #money 
              #life-hacking  </span>
              
              <h2 class="post-card-title">3 Ways I Spend Money to Improve Quality of Life</h2>
          </header>
          <section class="post-card-excerpt">
              
                <p>The illiterate of the future will not be the person who does not read. It will be the person who does not know how to learn - Alvin Toffler
I practice andragogy, which means I am constantly investing in my education as an adult, for betterment, fun, and naturally profit! This takes many forms, but one of the media I consume most is podcasts, and one of the podcasts I intently listen to is The Tim Ferriss Show. ...  </p>
              
          </section>
      </a>

      <footer class="post-card-meta">
          <img class="author-profile-image" src="/img/eddie_india_small.jpeg" alt="Author" />
          <span class="post-card-author"><a href="/">Eddie Forson</a></span>
      </footer>
    </div>
</article>
      
    </div>
  </div>
</aside>

<div class="floating-header">
  <div class="floating-header-logo">
    <a href="/">
      
      <span></span>
    </a>
  </div>
  <span class="floating-header-divider">&mdash;</span>
  <div class="floating-header-title">Using Computer Vision to Find Lane Lines on Road</div>
  <div class="floating-header-share">
    <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
     <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/></svg>
    </div>
    
    <a class="floating-header-share-tw" href="https://twitter.com/share?text=Using%20Computer%20Vision%20to%20Find%20Lane%20Lines%20on%20Road&amp;url=%2fposts%2fusing-computer-vision-to-find-lane-lines-on-road%2f"
          onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
      </a>
      <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=%2fposts%2fusing-computer-vision-to-find-lane-lines-on-road%2f"
          onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>
      </a>
  </div>

  <progress class="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</div>



<footer class="site-footer outer">
  <div class="site-footer-content inner">
    <section class="copyright" style="line-height: 1.3em;">
      <a href="/">EF</a>  <br>
      
    </section>
    <nav class="site-footer-nav">
        <a href="/">Latest Posts</a>
        
        <a href="https://twitter.com/Ed_Forson" target="_blank" rel="noopener">Twitter</a>
        <a href="https://github.com/kenshiro-o" target="_blank" rel="noopener">Github</a>
        <a href="https://www.linkedin.com/in/eddie-forson-16269b22" target="_blank" rel="noopener">LinkedIn</a>
        <a href="https://medium.com/@Ed_Forson" target="_blank" rel="noopener">Medium</a>
    </nav>  
  </div>
</footer>

</div>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script type="text/javascript" src="//code.jquery.com/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="/js/jquery.fitvids.js"></script>

<script>hljs.initHighlightingOnLoad();</script>



    <script>





$(document).ready(function () {
    
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>
</body></html>
